\documentclass[mathserif]{beamer}%
%
\xdef\sharedDir{../_shared_}%
\RequirePackage{\sharedDir/styles/slides}%
%
\subtitle{7. Simulated Annealing}%
%
\begin{document}%
\startPresentation%
%

\section{Introduction}%
%
\begin{frame}%
\frametitle{Local Search and Hill Climbing}%
\begin{itemize}%
\item So far, we have only discussed one variant of local search\only<-1>{.}\uncover<2->{: the stochastic hill climbing algorithm.}%
\item<3-> A pure hill climbing algorithm is likely to get stuck at local optima, which may vary in quality.%
\item<4-> We also found that we can utilize the variance of the result quality by restarting the optimization process when it could not improve any more.%
\item<5-> But such a restart is costly, as it forces the local search to start completely from scratch (while we, of course, remember the best-ever solution in a variable hidden from the algorithm).%
\item<6-> Alternatively, we also tried to use operators with larger neighborhoods\only<-6>{.}\uncover<7->{, %
but there will either still be local optima\uncover<8->{ %
(if the neighborhood is not large enough)\uncover<9->{ %
or there are points where it is hard to escape from\uncover<10->{ %
(if the neighborhood is very large but non-uniformly sampled, as our \algoStyle{nswap} operator does)\uncover<11->{ %
or the search will get very slow\uncover<12->{ %
(if the neighborhood is very large and uniformly sampled).%
}}}}}}%
\item<13-> So, for now, let's stick with the \algoStyle{1swap} operator.% 
\end{itemize}%
\end{frame}%
%
\begin{frame}%
\frametitle{Idea}%
\begin{itemize}%
\item A schedule which is a local optimum (under \algoStyle{1swap}) probably is at least somewhat similar to what the globally optimal schedule would look like.%
\item<2-> It must, obviously, also be somewhat different (otherwise it would be the global optimum already).%
\item<3-> This difference is shaped such that it cannot be conquered by the \algoStyle{1swap} unary search operator that we use.%
\item<4-> If we do a restart, we also dispose of the similarities to the global optimum that we have already discovered.%
\item<5-> Then, we will subsequently spend time to re-discover them in the hope that this will happen in a way that allows us to eventually reach the global optimum itself (or at least a better local optimum).%
\item<6-> Can there be a less-costly way?%
\end{itemize}%
\end{frame}%
%
\section{Algorithm Concept: Probabilistic Acceptance of Worse Solutions}%
%
\begin{frame}%
\frametitle{Simulated Annealing}%
\begin{itemize}%
\item Simulated Annealing (SA)\cite{KGV1983OBSA,C1985TATTTSPAESA,DPSW1982MCTICO,P1970AMCMFTASOCTOCOP} is a local search which provides another approach to escape from local optima\cite{WGOEB,S2003ITSSAO}.%
\item<2-> Instead of restarting the algorithm when reaching a local optima, it tries to preserve the parts of the current solution by sometimes permitting search steps towards worsening objective values.%
\item<3-> This algorithm therefore introduces three principles\uncover<4->{:%
\begin{enumerate}%
\item Worse candidate solutions are sometimes accepted, too.%
\item<5-> The probability~$P$ of accepting them is decreases with increasing differences~$\Delta E$ of the objective values to the current solution.%
\item<6-> The probability also decreases with the number of performed search steps.%
\end{enumerate}%
}%
\item<7-> These principles are "injected" into the basic loop of the hill climber.% 
\item<8-> How can we implement these concepts?%
\end{itemize}%
\end{frame}%
%
\newcounter{eqLast}%
\setcounter{eqLast}{\value{equation}}%
\newcounter{eqDeltaE}%
\setcounter{eqDeltaE}{\value{equation}}%
%
\begin{frame}%
\frametitle{Acceptance Probability}%
\begin{itemize}%
\item How can we implement these concepts?%
\item<2-> Let's assume that the ``current'' point in the search space known by our local search is~$\sespel\in\searchSpace$\only<-2>{.}\uncover<3->{ and that we have derived a new point~$\sespel'\in\searchSpace$ from it\only<-3>{.}\uncover<4->{ using the unary search operator.}}%
\item<5-> $\Delta E$ then be the difference between the objective value of~$\sespel'$ and~$\sespel$\only<-5>{.}\uncover<6->{:}%
\end{itemize}%
\uncover<6->{\bigskip%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
\Delta E = \objf(\repMap(\sespel')) - \objf(\repMap(\sespel))%
}}}%
\end{equation}%
\uncover<7->{%
\begin{itemize}%
\item $\Delta E < 0$ means that\only<-7>{?}\uncover<8->{ the new~$\sespel'$ is better than~$\sespel$ since $\objf(\repMap(\sespel')) < \objf(\repMap(\sespel))$.}%
\item<9-> $\Delta E > 0$ means that\only<-9>{?}\uncover<10->{ the new solution is worse.}%
\item<11-> $\Delta E = 0$ means that\only<-11>{?}\uncover<12->{ both have the same quality.}%
\end{itemize}%
}%
}%
\end{frame}%
%
\setcounter{eqLast}{\value{equation}}%
\newcounter{eqProbability}%
\setcounter{eqProbability}{\value{equation}}%
%
\begin{frame}%
\frametitle{Acceptance Probability}%
\setcounter{equation}{\value{eqDeltaE}}%
\begin{equation}%
{\color<10-13>{blue}{\Delta E}} = \objf(\repMap(\sespel')) - \objf(\repMap(\sespel))%
\end{equation}%
\uncover<2->{%
\vspace{-0.5em}%
\begin{itemize}%
\item The probability~$P$ to accept the new solution~$\sespel'$ (and discard the current one~$\sespel$) is\uncover<3->{:}% 
\end{itemize}%
\uncover<3->{\bigskip%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
P = \left\{\begin{array}{rl}%
\uncover<4->{1} & \uncover<5->{\text{if~}{\color<10-13>{blue}{\Delta E}} {\color<11>{violet}{\,\leq\,}} 0}\\%
\uncover<6->{e^{-\frac{{\color<10-13>{blue}{\Delta E}}}{\color<14>{red}{T}}}} & \uncover<7->{\text{if~}{\color<10-13>{blue}{\Delta E}} >0 \land {\color<14>{red}{T}} > 0}\\%
\uncover<8->{0} & \uncover<9->{\text{otherwise~}({\color<10-13>{blue}{\Delta E}} > 0 \land {\color<14>{red}{T}}=0)}%
\end{array} \right.%
}}}%
\label{equ:probability}%
\end{equation}%
\uncover<10->{%
\begin{itemize}%
\item If the new point~$\sespel'$ is better\only<11->{ {\color<11>{violet}{(or, at least, not worse)}}} than the current point~$\sespel$, i.e., ${\color<10-13>{blue}{\Delta E}} \only<-10>{<}\only<11->{{\color<11>{violet}{\,\leq\,}}}0$, then we will definitely accept it.%
\item<12-> If the new point~$\sespel'$ is worse (${\color<10-13>{blue}{\Delta E}} > 0$), then the acceptance probability\uncover<13->{%
\begin{enumerate}%
\item gets smaller the larger ${\color<10-13>{blue}{\Delta E}}$ is\only<-13>{.}\uncover<14->{ and}%
\item<14-> gets smaller the smaller the so-called ``temperature'' ${\color<14>{red}{T}}\geq 0$ is.%
\end{enumerate}%
}%
\end{itemize}%
}}}%
\end{frame}%
%
\section{Ingredient: Temperature Schedule}%
%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{Temperature Schedule}%
\setcounter{equation}{\value{eqProbability}}%
\begin{equation}%
P = \left\{\begin{array}{rl}%
1 & \text{if~}\Delta E \leq 0\\%
e^{-\frac{\Delta E}{{\only<-8>{\color<2>{red}{T}}\only<9->{\color<9>{cyan}{T(\iteration)}}}}} & \text{if~}\Delta E >0 \land {\only<-8>{\color<2>{red}{T}}\only<9->{\color<9>{cyan}{T(\iteration)}}} > 0\\%
0 & \text{otherwise~}(\Delta E > 0 \land {\only<-8>{\color<2>{red}{T}}\only<9->{\color<9>{cyan}{T(\iteration)}}}=0)%
\end{array}\right.%
\end{equation}%
\uncover<2->{%
\begin{itemize}%
\item What about this temperature~${\only<-8>{\color<2>{red}{T}}\only<9->{\color<9>{cyan}{T(\iteration)}}}$?%
\item<3-> The temperature is defined to decrease and approaches zero with a rising number~$\iteration$ of \only<-3>{algorithm iterations, i.e., the }performed objective function evaluations.
\item<4-> The optimization process is initially ``hot'' and $\only<-8>{T}\only<9->{{\color<9>{cyan}{T(\iteration)}}}$~is high.%
\item<5-> Then, even significantly worse solutions may be accepted.%
\item<6-> Over time, the process ``cools'' down and $\only<-8>{T}\only<9->{{\color<9>{cyan}{T(\iteration)}}}$~decreases.%
\item<7-> Slowly, fewer and fewer worse solutions are accepted and more likely such which are only a bit worse.%
\item<8-> At temperature~$\only<-8>{T}\only<9->{{\color<9>{cyan}{T(\iteration)}}}=0$, the algorithm only accepts better solutions.% 
\item<9-> \alert{$T$ is a monotonously decreasing function ${\color<9>{cyan}{T(\iteration)}}$: the ``temperature schedule.''}%
\end{itemize}%
}%
\end{frame}%
%
\begin{frame}%
\frametitle{Conditions for Temperature Schedule}%
\setcounter{equation}{\value{eqProbability}}%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
P = \left\{\begin{array}{rl}%
1 & \text{if~}\Delta E \leq 0\\%
e^{-\frac{\Delta E}{{\color{cyan}{T(\iteration)}}}} & \text{if~}\Delta E >0 \land {\color{cyan}{T(\iteration)}} > 0\\%
0 & \text{otherwise~}(\Delta E > 0 \land {\color{cyan}{T(\iteration)}}=0)%
\end{array}\right.%
}}}%
\end{equation}%
\uncover<2->{%
\begin{itemize}%
\item The temperature~${\color{cyan}{T(\iteration)}}$ is defined to decrease and approaches zero with a rising number~$\iteration$ of performed objective function evaluations.%
\item<3-> It holds that $\displaystyle{\lim_{\iteration\rightarrow+\infty} {\color{cyan}{T(\iteration)}} = 0}$.%
\item<4-> It begins with an start temperature~$T_s$ at $\iteration=1$.%
\item<5-> Apart from this, we can define~${\color{cyan}{T(\iteration)}}$ in any way we want.%
\end{itemize}%
}%
\end{frame}%
%
\begin{frame}%
\frametitle{Base Class for Implementing Temperature Schedules}%
\listing{0.9}{0.9}{language=myJava,mathescape=true}{code/TemperatureSchedule.java}
\end{frame}%
%
\newcounter{eqExponential}%
\setcounter{eqExponential}{\value{equation}}%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{Exponential Temperature Schedule}%
\begin{itemize}%
\item In an \alert{exponential temperature schedule}, the temperature decreases exponentially with time (as the name implies).%
\item<2-> Besides the start temperature~$T_s$, it has a parameter~$\varepsilon\in(0,1)$ which tunes the speed of the temperature decrease.%
\uncover<3->{%
\bigskip%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
T(\iteration) = T_s * (1 - \varepsilon) ^ {\iteration - 1}%
}}}%
\label{equ:exponential}%
\end{equation}%
}%
\item<4-> Higher values of $\varepsilon$ lead to a faster temperature decline.
\end{itemize}%
\end{frame}%
%
\begin{frame}%
\frametitle{Exponential Temperature Schedule}%
\listing{0.9}{0.9}{language=myJava,mathescape=true}{code/TemperatureSchedule_Exponential.java}
\end{frame}%
%
\newcounter{eqLogarithmic}%
\setcounter{eqLogarithmic}{\value{equation}}%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{Logarithmic Temperature Schedule}%
\begin{itemize}%
\item The \alert{logarithmic temperature schedule} will prevent the temperature from becoming very small for a longer time.%
\item<2-> Compared to the exponential schedule, it will longer retain a higher probability to accept worse solutions.%
\item<3-> It also has the parameters~$\varepsilon\in(0,\infty)$ and~$T_s$.
\uncover<4->{%
\bigskip%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
T(\iteration) = \frac{T_s}{\ln{\left(\varepsilon(\iteration-1)+e\right)}}%
}}}%
\end{equation}%
}%
\item<5-> Larger values of $\varepsilon$ again lead to a faster temperature decline.%
\end{itemize}%
\end{frame}%
%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{Logarithmic Temperature Schedule}%
\listing{0.9}{0.9}{language=myJava,mathescape=true}{code/TemperatureSchedule_Logarithmic.java}
\end{frame}%
%
\begin{frame}%
\frametitle{The Meaning of the Temperature Schedule}%
\begin{itemize}%
\item Why do we have such a strange thing like a temperature schedule?%
\item<2-> Let's think back again about Evolutionary Algorithms\cite{WGOEB,H1975GA,BFM1997EA,MF2004HTSIMH,G1989GA}.%
\item<3-> By using the population size parameters~$\mu$ and $\lambda$, we can tune the behavior of an EA between random sampling ($\mu\rightarrow\infty$ or $\lambda\rightarrow\infty$) and hill climbing ($\mu=\lambda=1$).%
\item<4-> This allowed us to tune between exploration and exploitation, to find a ``sweet spot'' where the algorithm performs best.%
\item<5-> The temperature schedule in SA allows us to do the same\only<-5>{.}\uncover<6->{ \alert<6>{but dynamically!}}%
\item<7-> If $T$ is high at the beginning\only<-7>{\dots}\uncover<8->{ $\Rightarrow$ many bad solutions are accepted\only<-8>{.}\uncover<9->{ $\Rightarrow$ random sampling.}}%
\item<10-> At the end, $T\approx 0$\only<-10>{\dots}\uncover<11->{ $\Rightarrow$ no worse solutions are accepted anymore\only<-11>{.}\uncover<12->{ $\Rightarrow$ hill climbing.}}%
\end{itemize}%
\end{frame}%
%
\section{Algorithm Implementation}%
%
\begin{frame}%
\frametitle{Simulated Annealing Algorithm}%
\begin{itemize}%
\item Simulated Annealing = Hill Climbing + probabilistically accepting worse solutions%
\item<2-> Simple Concept\uncover<3->{:%
\begin{enumerate}%
\item Start with ${\color{cyan}{\iteration}}=1$.
\item<4-> Create random initial point~${\color{green}{\sespel}}$, which also becomes the first ``current point''~${\color{green}{\sespel}}$ and the overall best point~${\color{red}{\bestSoFar{\sespel}}}$.%
\item<5-> Create a modified copy~${\color{blue}{\sespel'}}$ of the current point~${\color{green}{\sespel}}$.%
\item<6-> Set ${\color{cyan}{\iteration}}={\color{cyan}{\iteration}}+1$.%
\item<7-> If the new point~${\color{blue}{\sespel'}}$ is better than~${\color{red}{\bestSoFar{\sespel}}}$, set~${\color{red}{\bestSoFar{\sespel}}}={\color{blue}{\sespel'}}$.%
\item<8-> If the new point~${\color{blue}{\sespel'}}$ is better than~${\color{green}{\sespel}}$, set~${\color{green}{\sespel}}={\color{blue}{\sespel'}}$.%
\item<9-> If it is worse ($\Delta E>0$): accept it as current solution with probability~$P(\Delta E, {\color{cyan}{\iteration}})$ (which gets smaller over time and also the smaller the worse the new solution is) or otherwise reject it.%
\item<10-> Go back to \enumerateItem{3} (until the time is up)%
\item<11-> Return the best ever-encountered point~${\color{red}{\bestSoFar{\sespel}}}$.%
\end{enumerate}}
\end{itemize}%
\end{frame}%
%
\begin{frame}%
\frametitle{Implementing Simulated Annealing}%
\only<1>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_01.java}}%
\only<2>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_02.java}}%
\only<3>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_03.java}}%
\only<4>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_04.java}}%
\only<5>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_05.java}}%
\only<6>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_06.java}}%
\only<7>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_07.java}}%
\only<8>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_08.java}}%
\only<9>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_09.java}}%
\only<10>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_10.java}}%
\only<11>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_11.java}}%
\only<12>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_12.java}}%
\only<13>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_13.java}}%
\only<14>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_14.java}}%
\only<15>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing_15.java}}%
\only<16>{\listing{0.9}{1.37}{language=myJava,mathescape=true}{code/SimulatedAnnealing.java}}%
\end{frame}%
%
\section{Configuring the Algorithm}%
%
\begin{frame}%
\frametitle{Configuring the Algorithm}%
\begin{itemize}%
\item Our algorithm has four parameters\only<-1>{.}\uncover<2->{:%
\begin{enumerate}%
\item the start temperature~$T_s$\uncover<3->{,}%
\item<3-> the parameter~$\varepsilon$\uncover<4->{,}%
\item<4-> the type of temperature schedule to use (here, logarithmic or exponential)\uncover<5->{, and}%
\item<5-> the unary search operator (in our case, we could use \algoStyle{1swap} or \algoStyle{nswap}).%
\end{enumerate}}%
\item<6-> We will only use \algoStyle{1swap} as choice for the unary operator and focus on the exponential temperature schedule.%
\item<7-> This leaves $T_s$ and $\varepsilon$ to be configured.%
\item<8-> Interestingly, we may be able to \alert{very roughly compute} some reasonable values for them!%
\end{itemize}%
\end{frame}%
%
\begin{frame}%
\frametitle{Simulated Annealing as Improved Hill Climber}%
\begin{itemize}%
\item Let us consider Simulated Annealing as an improved Hill Climber\only<-1>{.}\uncover<2->{ and look at the experimental results of this algorithm.}%
\end{itemize}%
\uncover<3->{%
\bigskip%
\begin{center}%
\begin{tabular}{|r|r|r|}%
\hline%
$\instance$&\textcolor<4>{blue}{med(total FEs)}&\textcolor<5>{red}{sd}\\%
\hline%
\instStyle{abz7}&\textcolor<4>{blue!66!black}{35'648'639}&28\\%
\instStyle{la24}&70'952'285&\textcolor<5>{red!66!black}{56}\\%
\instStyle{swv15}&21'662'286&137\\%
\instStyle{yn4}&\textcolor<4>{blue!66!black}{27'090'511}&\textcolor<5>{red!66!black}{48}\\%
\hline%
\textcolor<4-5>{violet}{\textbf{median}}&\textcolor<4>{blue}{\textbf{31'369'575}}&\textcolor<5>{red}{\textbf{52}}\\%
\hline%
\end{tabular}%
\end{center}%
\uncover<4->{%
\bigskip%
\begin{itemize}%
\item \algoStyle{hc_1swap} performs \textcolor<4>{blue}{30 million FEs} (within the three minute budget) in \textcolor<4>{violet}{median over all instances}.%
\item<5-> The \textcolor<5>{violet}{median} of the standard deviations of the result quality at the end of the three minutes (\textcolor<5>{violet}{over all instances}) is about \textcolor<5>{red}{50}.%
\item<6-> What can we do with these information?% 
\end{itemize}%
}%
}%
\end{frame}%
%
\begin{frame}%
\frametitle{End Result Standard Deviation}%
\only<-2,7->{%
\begin{itemize}%
\item The median standard deviation of the final results of \algoStyle{hc_1swap} of~50 tells us something about the local optima.%
\item<2-> We know that \algoStyle{hc_1swap} gets stuck in local optima -- it stopped improving after just one second!%
\item<7-> The standard measures how spread out the local optima.%
\item<8-> It is a gives us a good impression of how different the qualities of the local optima are that we can expect to see.%
\item<9-> Thus, accepting a solution which is worse by 50~units of makespan, i.e., with $\Delta E\approx 50$, should be possible at the beginning of the optimization process.%
\item<10-> Let's say that the probability to accept such a solution should be 10% at the beginning of a run.%  
\end{itemize}%
}%
\locateGraphic{3}{width=0.9\paperwidth}{../05_stochastic_hill_climbing/graphics/hc_1swap_progress/hc_1swap_progress_abz7_log}{0.05}{0.2}%
\locateGraphic{4}{width=0.9\paperwidth}{../05_stochastic_hill_climbing/graphics/hc_1swap_progress/hc_1swap_progress_la24_log}{0.05}{0.2}%
\locateGraphic{5}{width=0.9\paperwidth}{../05_stochastic_hill_climbing/graphics/hc_1swap_progress/hc_1swap_progress_swv15_log}{0.05}{0.2}%
\locateGraphic{6}{width=0.9\paperwidth}{../05_stochastic_hill_climbing/graphics/hc_1swap_progress/hc_1swap_progress_yn4_log}{0.05}{0.2}%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{From End Result Standard Deviation to Start Temperature}%
\begin{itemize}%
\item The median standard deviation of the end result quality of the hill climber is~50.%
\item<2-> We want to accept a solution with $\Delta E=50$ with probability $P_{50}=0.1$ at~$\iteration=1$.%
\item<3-> At $\iteration=1$, the temperature of any temperature schedule equals the start temperature~$T_s$.%
\item<4-> We can solve the probability Equation~\ref{equ:probability} for $T_s$\uncover<5->{:}% 
\end{itemize}%
\uncover<4->{%
\bigskip%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
\only<-5>{%
P = \left\{\begin{array}{rl}%
1 & \text{if~}\Delta E \leq 0\\%
{\color<5>{red}{e^{-\frac{\Delta E}{T(\iteration)}}}} & {\color<5>{red}{\text{if~}\Delta E >0 \land T(\iteration) > 0}}\\%
0 & \text{otherwise~}(\Delta E > 0 \land T(\iteration)=0)%
\end{array} \right.%
}%
}}}\scalebox{1.6}{\ensuremath{\displaystyle{%%
\only<6-7>{%
{\color<7>{blue}{P}}= e^{-\frac{\Delta E}{T(\iteration)}}%
}%
\only<8>{%
{\color<8>{blue}{P_{50}}}= e^{-\frac{\Delta E}{T(\iteration)}}%
}%
\only<9-10>{%
{\color<9>{blue}{0.1}}= e^{-\frac{{\color<10>{red}{\Delta E}}}{T(\iteration)}}%
}%
\only<11-12>{%
0.1 = e^{-\frac{ {\color<11>{red}{50}} }{ {\color<12>{blue}{T(\iteration)}} }}%
}%
\only<13-14>{%
0.1 = {\color<14>{red}{e}}^{-\frac{ 50 }{ {\color<13>{blue}{T_s}} } }%
}%
\only<15-16>{%
{\color<16>{blue}{{\color<15>{red}{\ln{0.1}}}}} = -\frac{50}{{\color<16>{blue}{T_s}}}%
}%
\only<17-18>{%
{\color<17>{blue}{T_s}} = {\color<18>{red}{ -\frac{50}{{\color<17>{blue}{\ln{0.1}}}}}}%
}%
\only<19>{%
T_s\approx {\color<19>{red}{21.714\,724\,095}}
}%
\only<20->{%
T_s\approx 20%
}%
}}}%
\end{equation}%
\uncover<21->{%
\begin{itemize}%
\item A start temperature~$T_s$ of about~20 seems to be a good choice.%
\end{itemize}%
}%
}%
\end{frame}%
%
\begin{frame}%
\frametitle{End Temperature}%
\begin{itemize}%
\item Let us first think about the end temperature~$T_e$ that should be reached at the end of the run.%
\item<2-> While we know that $\displaystyle{\lim_{\iteration\rightarrow+\infty} T(\iteration)=0}$, we also know that three minutes of runtime is less than $+\infty$.
\item<3-> $T=0$ will thus not be reached within a finite number~$\iteration$ of steps and the actual end temperature~$T_e$ should probably be slightly above~0.%
\item<4-> Let us remember back our \algoStyle{hcr_L_1swap} algorithm, i.e., the \algoStyle{1swap} hill climber restarting after $L$~unsuccessful steps.%
\item<5-> There, we found $L=2^{14}=16'384$ to be reasonable choice.%
\item<6-> As idea to get a reasonable~$T_e$, we could say that the end probability~$P_e$ to accept a solution which is $\Delta E=1$~makespan unit worse than the current solution should be $P_e=1/L=\tfrac{1}{16'384}$ at the end of our Simulated Annealing runs.%
\item<7-> Then, the chance to accept a solution marginally worse than the current one would be about as large as making a complete restart in~\algoStyle{hcr_16384_1swap}.%
\item<8-> This is a bit far fetched\only<-8>{.}\uncover<9->{, but as a rough guess it will do.}%
\end{itemize}%
\end{frame}%
%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{End Temperature}%
\begin{itemize}%
\item To get an {\color<8-9>{red}{end temperature~$T_e$}}, the end probability~{\color<6-7>{blue}{$P_e$}} to accept a solution which is {\color<10-11>{blue}{$\Delta E=1$}}~makespan unit worse than the current solution should be {\color<7>{blue}{$P_e=1/L=\tfrac{1}{16'384}$}} at the end of our Simulated Annealing runs.%
\end{itemize}%
\uncover<2->{%
\bigskip%
\begin{equation}%
\scalebox{1.3}{\ensuremath{\displaystyle{%
\only<-3>{%
P = \left\{\begin{array}{rl}%
1 & \text{if~}\Delta E \leq 0\\%
{\color<3>{red}{e^{-\frac{\Delta E}{T(\iteration)}}}} & {\color<3>{red}{\text{if~}\Delta E >0 \land T(\iteration) > 0}}\\%
0 & \text{otherwise~}(\Delta E > 0 \land T(\iteration)=0)%
\end{array} \right.%
}%
}}}\scalebox{1.6}{\ensuremath{\displaystyle{%%
\only<4-5>{%
{\color<5>{blue}{P}} = {\color<4>{red}{e^{-\frac{\Delta E}{T(\iteration)}}}}%
}%
\only<6>{%
{\color<6>{blue}{P_e}} = e^{-\frac{\Delta E}{T(\iteration)}}%
}%
\only<7-8>{%
{\color<7>{blue}{\frac{1}{16'384}}} = e^{-\frac{\Delta E}{{\color<8>{red}{T(\iteration)}}}}%
}%
\only<9-10>{%
\frac{1}{16'384} = e^{-\frac{{\color<10>{blue}{\Delta E}}}{{\color<9>{red}{T_e}}}}%
}%
\only<11-12>{%
\frac{1}{16'384} = {\color<12>{red}{e}}^{-\frac{{\color<11>{blue}{1}}}{T_e}}%
}%
\only<13-14>{%
{\color<14>{blue}{{\color<13>{red}{\ln{\frac{1}{16'384}}}}}} = -\frac{1}{{\color<14>{blue}{T_e}}}%
}%
\only<15-16>{%
{\color<15>{blue}{T_e}} = {\color<16>{red}{-\frac{1}{{\color<15>{blue}{\ln{\frac{1}{16'384}}}}}}}%
}%
\only<17>{%
T_e \approx {\color<17>{red}{0.103\,049\,646}}%
}%
\only<18->{%
T_e \approx {\color<18>{red}{0.1}}%
}%
%
}}}%
\end{equation}%
\uncover<19->{
\begin{itemize}%
\item It seems that an end temperature~$T_e\approx=0.1$ is a reasonable setting for SA using \algoStyle{1swap}.%
\end{itemize}%
}%
}%
\end{frame}%
%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{Epsilon from End Temperature and Iteration}%
\begin{itemize}%
\item We now want to find a good setting for the~$\varepsilon$ parameter.%
\item<2-> This parameter plays a role in the exponential temperature schedule.%
\item<4-> It relates the temperature~$T(\iteration)$ at a given iteration~$\iteration$ to the iteration index~$\iteration$.%
\item<5-> In order to compute a rough guess for~$\varepsilon$, we thus need a value for~$\iteration$ and one for~$T(\iteration)$ first.%
\item<6-> The start temperature~$T_s$ alone does not help us here, but we now also have an end temperature~$T_e$.%
\end{itemize}%
\uncover<3->{%
\bigskip%
\setcounter{equation}{\value{eqExponential}}%
\begin{equation}%
\scalebox{1.6}{\ensuremath{\displaystyle{%
T(\iteration) = T_s * (1 - \varepsilon) ^ {\iteration - 1}%
}}}%
\end{equation}%
}%
\end{frame}%
%
\setcounter{equation}{\value{eqLast}}%
%
\begin{frame}%
\frametitle{Epsilon from End Temperature and Iteration}%
\begin{itemize}%
\item We have a start temperature~$T_s$ and an end temperature~$T_e$.%
\item<2-> What we need it we want to solve Equation~\ref{equ:exponential} for~$\varepsilon$ is the iteration index~$\iteration$ at which~$T(\iteration)=T_e$.%
\item<3-> Before, we said that our optimization processes run for about 30'000'000~FEs in median.%
\item<4-> Since $T_e$~is the end temperature, the right value for~$\iteration$ is the time when we can expect the runs to end: $T_e=T(30'000'000)$ and~$\iteration=30'000'000$.%
\end{itemize}%
\uncover<4->{%
\bigskip%
\begin{equation}%
\scalebox{1.6}{\ensuremath{\displaystyle{%
%
\only<-5>{%
{\color<5>{red}{T(\iteration)}} = T_s * (1 - \varepsilon) ^ {\iteration - 1}%
}%
\only<6>{%
{\color<6>{red}{T_e}} = T_s * (1 - \varepsilon) ^ {\iteration - 1}%
}%
\only<7-8>{%
{\color<7>{red}{0.1}} = T_s * (1 - \varepsilon) ^ {{\color<8>{blue}{\iteration}} - 1}%
}%
\only<9-10>{%
0.1 = T_s * (1 - \varepsilon) ^ {{\color<10>{red}{{\color<9>{blue}{30'000'000}} - 1}}}%
}%
\only<11-12>{%
0.1 = {\color<12>{red}{T_s}} * (1 - \varepsilon) ^ {{\color<11>{red}{29'999'999}}}%
}%
\only<13-14>{%
{\color<14>{blue}{0.1 = {\color<13>{red}{20}}}} * (1 - \varepsilon) ^ {29'999'999}%
}%
\only<15>{%
{\color<15>{blue}{\frac{0.1}{20}}} = (1 - \varepsilon) ^ {29'999'999}%
}%
\only<16-17>{%
{\color<16>{blue}{0.005}} = (1 - \varepsilon) ^ {{\color<17>{red}{29'999'999}}}%
}%
\only<18>{%
{\color<18>{red}{0.005^{1/29'999'999}}} = 1 - \varepsilon%
}%
\only<19>{%
{\color<19>{red}{ 0.999\,999\,823\,389  \approx}}\,{\color<20>{blue}{1 -}} \varepsilon%
}%
\only<20-21>{%
\varepsilon\approx{\color<21>{red}{1-{\color<20>{blue}{0.999\,999\,823\,389}}}}%
}%
\only<22>{%
\varepsilon\approx{\color<22>{red}{0.000\,000\,176\,610\,569}}%
}%
\only<23>{%
\varepsilon\approx{\color<23>{red}{1.776 * 10^{-7}}}%
}%%
\only<24->{%
\varepsilon\,{\color<24>{red}{\in \left[1 * 10^{-7}, 2*10^{-7}\right]}}%
}%
%
}}}%
\end{equation}%
}%
%
\uncover<25->{%
\begin{itemize}%
\item Values of~$\varepsilon$ between 1~and 2~times $10^{-7}$ seem reasonable.%
\end{itemize}%
}%
\end{frame}%
%
\setcounter{eqLast}{\value{equation}}%
%
\begin{frame}%
\frametitle{Configuration from Previous Knowledge}%
\begin{itemize}%
\item We now have reasonable parameter values for our Simulated Annealing algorithm with Exponential Temperature Schedule.%
\item<2-> We have a rough impression about how far local optima under the unary operator are apart in terms of objective value (about~50).%
\item<3-> We used this to obtain a reasonable start \mbox{temperature $T_s=20$.}%
\item<4-> We can choose a reasonably small end temperature~$T_e$.
\item<5-> We did this by setting $T_e=0.1$ such that we would accept a solution which is $\Delta E=1$ worse than the current solution about every $L=16'384$ steps (which was the length until the hill climber would do a restart).%
\item<6-> Finally, by knowing that we can do about 30'000'000~FEs in total, we can set $\varepsilon\in\left[1 * 10^{-7}, 2*10^{-7}\right]$ such that $T_e$ would be reached near the end of the runs.%
\end{itemize}%
\end{frame}%
%
\begin{frame}[b]%
\frametitle{Behavior of the Configurations}%
\locateGraphic{1,6-}{width=0.98\paperwidth}{graphics/sa_temperature_schedules/sa_temperature}{0.0015}{0.14}%
\locateGraphic{2}{width=0.98\paperwidth}{graphics/sa_temperature_schedules/sa_probability_1}{0.0015}{0.14}%
\locateGraphic{3}{width=0.98\paperwidth}{graphics/sa_temperature_schedules/sa_probability_3}{0.0015}{0.14}%
\locateGraphic{4}{width=0.98\paperwidth}{graphics/sa_temperature_schedules/sa_probability_10}{0.0015}{0.14}%
\locateGraphic{5}{width=0.98\paperwidth}{graphics/sa_temperature_schedules/sa_probability_50}{0.0015}{0.14}%
%
\uncover<6->{%
\begin{itemize}%
\item Our very rough calculations gave us parameter settings for~$T_s$ and~$\varepsilon$ that produce these temperature- and probability curves.%
\item<7-> Whether these settings are actually any good must be studied now.%
\end{itemize}%
}%
\end{frame}%
%
\begin{frame}%
\frametitle{Relation of \texorpdfstring{$\varepsilon$}{Epsilon} and Performance}%
\locateGraphic{}{width=0.99\paperwidth}{graphics/sa_config/sa_1swap_med_over_epsilon}{0.005}{0.15}%
\locate{2-}{%
\parbox{0.55\paperwidth}{%
\begin{itemize}%
\item Indeed, values of $\varepsilon\in\left[1 * 10^{-7}, 2*10^{-7}\right]$ perform well for $T_s=20$.%
\item<3-> Only for \instStyle{la24}, smaller~$\varepsilon$ are better\only<-3>{.}\uncover<4->{, because on \instStyle{la24}, we could do more than 70~million FEs, whereas on all other instances, we did less than 36~million.}% 
\end{itemize}%
}}{0.4}{0.28}%
\end{frame}%
%
\section{Experiment and Analysis}%
%
\begin{frame}[t]%
\frametitle{So what do we get?}%
%
\only<3->{%
\begin{center}%
\only<3,5,7,9>{\small{\algoStyle{eac_4_5\%_nswap}: median result of 3~min of the EA with clearing and $\mu=\lambda=4$ with \algoStyle{nswap} unary operator and 5\% \algoStyle{sequence} recombination}}%
\only<4,6,8,10>{\small{\algoStyle{sa_exp_20_2_1swap}: median result of 3~min of Simulated Annealing with exponential schedule, $T_s=20$, and $\varepsilon=2\cdot10^{-7}$ and \algoStyle{1swap} unary operator}}%
\end{center}%
}%
%
\only<-2>{%
\begin{itemize}%
\item I execute the program 101~times for each of the instances \instStyle{abz7}, \instStyle{la24}, \instStyle{swv15}, and \instStyle{yn4}%
\end{itemize}%
\uncover<2->{%
\begin{center}%
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{|l|l|r|r|r|r|r|r|}%
\hline%
&&\multicolumn{4}{c|}{\textbf{makespan}}&\multicolumn{2}{c|}{\textbf{last improvement}}\\%
\hline%
$\instance$&algo&best&mean&med&sd&med(t)&med(FEs)\\%
\hline%
\hline%
\instStyle{abz7}&\algoStyle{hcr_65536_nswap}&712&731&732&6&96s&21'189'358\\%
&\algoStyle{eac_4_5\%_nswap}&672&690&690&9&68s&12'474'571\\%
&\algoStyle{sa_exp_20_2_1swap}&\textcolor{green}{\textbf{663}}&\textcolor{green}{\textbf{673}}&\textcolor{green}{\textbf{673}}&\textcolor{green}{\textbf{5}}&112s&21'803'600\\%
\hline%
\instStyle{la24}&\algoStyle{hcr_65536_nswap}&942&973&974&\textcolor{green}{\textbf{8}}&71s&31'466'420\\%
&\algoStyle{eac_4_5\%_nswap}&\textcolor{green}{\textbf{935}}&963&961&16&30s&9'175'579\\%
&\algoStyle{sa_exp_20_2_1swap}&938&\textcolor{green}{\textbf{949}}&\textcolor{green}{\textbf{946}}&8&33s&12'358'941\\%
\hline%
\instStyle{swv15}&\algoStyle{hcr_65536_nswap}&3740&3818&3826&35&89s&10'783'296\\%
&\algoStyle{eac_4_5\%_nswap}&3102&3220&3224&65&168s&18'245'534\\%
&\algoStyle{sa_exp_20_2_1swap}&\textcolor{green}{\textbf{2936}}&\textcolor{green}{\textbf{2994}}&\textcolor{green}{\textbf{2994}}&\textcolor{green}{\textbf{28}}&157s&20'045'507\\%
\hline%
\instStyle{yn4}&\algoStyle{hcr_65536_nswap}&1068&1109&1110&12&78s&18'756'636\\%
&\algoStyle{eac_4_5\%_nswap}&1000&1038&1037&18&118s&15'382'072\\%
&\algoStyle{sa_exp_20_2_1swap}&\textcolor{green}{\textbf{973}}&\textcolor{green}{\textbf{985}}&\textcolor{green}{\textbf{985}}&\textcolor{green}{\textbf{5}}&130s&20'407'559\\%
\hline%
\end{tabular}%
}%
\end{center}%
}%
}%
%
\locateGraphic{3}{width=0.95\paperwidth}{../06_evolutionary_algorithm/graphics/eac_gantt/gantt_eac_abz7_690}{0.025}{0.26}%
\locateGraphic{4}{width=0.95\paperwidth}{graphics/sa_gantt/gantt_sa_20_2em7_abz7_690}{0.025}{0.26}%
\locateGraphic{5}{width=0.95\paperwidth}{../06_evolutionary_algorithm/graphics/eac_gantt/gantt_eac_la24_961}{0.025}{0.26}%
\locateGraphic{6}{width=0.95\paperwidth}{graphics/sa_gantt/gantt_sa_20_2em7_la24_961}{0.025}{0.26}%
\locateGraphic{7}{width=0.95\paperwidth}{../06_evolutionary_algorithm/graphics/eac_gantt/gantt_eac_swv15_3224}{0.025}{0.26}%
\locateGraphic{8}{width=0.95\paperwidth}{graphics/sa_gantt/gantt_sa_20_2em7_swv15_3224}{0.025}{0.26}%
\locateGraphic{9}{width=0.95\paperwidth}{../06_evolutionary_algorithm/graphics/eac_gantt/gantt_eac_yn4_1037}{0.025}{0.26}%
\locateGraphic{10}{width=0.95\paperwidth}{graphics/sa_gantt/gantt_sa_20_2em7_yn4_1037}{0.025}{0.26}%
\end{frame}%
%
\begin{frame}[t]%
\frametitle{Progress over Time}%
\locateGraphic{2}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_abz7_log}{0.05}{0.2}%
\locateGraphic{3}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_abz7}{0.05}{0.2}%
\locateGraphic{4}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_la24_log}{0.05}{0.2}%
\locateGraphic{5}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_la24}{0.05}{0.2}%
\locateGraphic{6}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_swv15_log}{0.05}{0.2}%
\locateGraphic{7}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_swv15}{0.05}{0.2}%
\locateGraphic{8}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_yn4_log}{0.05}{0.2}%
\locateGraphic{9}{width=0.9\paperwidth}{graphics/sa_progress/sa_progress_yn4}{0.05}{0.2}%
\begin{center}%
What progress does the algorithm make over time?
\par%
\vspace{0.65\paperheight}%
\only<9->{Simulated Annealing is better than the other algorithms and keeps improving longer.}%
\end{center}
\end{frame}%
%
\begin{frame}[t]%
\frametitle{Optimal Solutions for \texorpdfstring{\instStyle{la24}}{la24}}%
\only<-2>{%
\begin{itemize}%
\item Interestingly, the setups with~$\varepsilon=4\cdot10^{-7}$ and~$\varepsilon=8\cdot10^{-7}$, which we did not choose for our summary, each found one solution for \instStyle{la24} with makespan~935.%
\item<2-> Since we know that the lower bound for the makespan on \instStyle{la24} is also~935\cite{AC1991ACSOTJSSP,vH2015JSIAS}, we know that we found two globally optimal, best possible solutions!%
\end{itemize}%
}%
\only<3->{%
\begin{center}%
\only<-3>{\small{\algoStyle{sa_exp_20_2_1swap}: median result of 3~min of Simulated Annealing with exponential schedule, $T_s=20$, and $\varepsilon=2\cdot10^{-7}$ and \algoStyle{1swap} unary operator}}%
\only<4>{\small{\algoStyle{sa_exp_20_4_1swap}: \alert{best} result of 3~min of Simulated Annealing with exponential schedule, $T_s=20$, and $\varepsilon=4\cdot10^{-7}$ and \algoStyle{1swap} unary operator}}%
\only<5>{\small{\algoStyle{sa_exp_20_8_1swap}: \alert{best} result of 3~min of Simulated Annealing with exponential schedule, $T_s=20$, and $\varepsilon=8\cdot10^{-7}$ and \algoStyle{1swap} unary operator}}%
\end{center}%
}%
%
\locateGraphic{3}{width=0.95\paperwidth}{graphics/sa_gantt/gantt_sa_20_2em7_la24_946}{0.025}{0.26}%
\locateGraphic{4}{width=0.95\paperwidth}{graphics/sa_gantt_optimal/gantt_sa_20_4em7_la24_opt_946}{0.025}{0.26}%
\locateGraphic{5}{width=0.95\paperwidth}{graphics/sa_gantt_optimal/gantt_sa_20_8em7_la24_opt_946}{0.025}{0.26}%
\end{frame}%
%
%
\begin{frame}%
\frametitle{Summary}%
\begin{itemize}%
%
\item Simulated Annealing outperformed all algorithms that we have tested before.%
%
\item<2-> We can also use knowledge to configure the algorithm more quickly.\uncover<3->{ For this, we need to know approximately:%
\begin{itemize}%
%
\item approximately how many algorithm steps we can do within the computational budget\only<-3>{.}\uncover<4->{ and}%
%
\item<4-> what ``a bit worse'' means in terms of the objective function.%
%
\item<5-> We then can determine a starting temperature~$T_s$ and a parameter~$\epsilon$ to tune the temperature schedule accordingly.%
\end{itemize}}%
%
\item<6-> Perspective\uncover<7->{:%
\begin{itemize}%
\item An Evolutionary Algorithm allows us to pick a behavior in between a hill climber and a random sampling algorithm by choosing a small or large population size.%
%
\item<8-> The Simulated Annealing algorithm allows for a smooth transition of a random search behavior towards a hill climbing behavior over time.%
%
\item<9-> Compared to the hill climber with restarts, it offers a ``softer'' way to escape from local optima which sacrifices less solution information.%
\end{itemize}%
}%
\end{itemize}%
\end{frame}%
%
\endPresentation%
\end{document}%%
\endinput%
%
